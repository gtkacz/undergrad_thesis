{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tomllib\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import PIL\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "from util import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'TRAINING': {'healthy_skin_path': './dataset/healthy/',\n",
       "  'diseased_skin_path': './dataset/psoriasis/',\n",
       "  'training_dataset_ratio': 0.8,\n",
       "  'pretrained_model_name': 'youngp5/skin-conditions',\n",
       "  'target_label': 'Psoriasis pictures Lichen Planus and related diseases',\n",
       "  'threshold': 0.9}}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('parameters.toml', 'r') as f:\n",
    "    parameters = tomllib.loads(f.read())\n",
    "    \n",
    "parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total dataset: 2806 items.\n",
      "\n",
      "Training: 2245 items.\n",
      "Validation: 281 items.\n",
      "Testing: 280 items.\n"
     ]
    }
   ],
   "source": [
    "base_dataset = {'training': list(), 'validation': list(), 'testing': list()}\n",
    "\n",
    "raw_healthy_images = read_images(parameters['TRAINING']['healthy_skin_path'])\n",
    "raw_diseased_images = read_images(parameters['TRAINING']['diseased_skin_path'])\n",
    "\n",
    "total_raw_images = raw_healthy_images + raw_diseased_images\n",
    "\n",
    "random.shuffle(total_raw_images)\n",
    "\n",
    "for i, image in enumerate(total_raw_images):\n",
    "    training_ratio = parameters['TRAINING']['training_dataset_ratio']\n",
    "\n",
    "    if i < len(total_raw_images) * training_ratio:\n",
    "        base_dataset['training'].append(image)\n",
    "    elif i < len(total_raw_images) * round((1 + training_ratio) / 2, 1):\n",
    "        base_dataset['validation'].append(image)\n",
    "    else:\n",
    "        base_dataset['testing'].append(image)\n",
    "        \n",
    "print(f\"Total dataset: {len(total_raw_images)} items.\\n\")\n",
    "print(f\"Training: {len(base_dataset['training'])} items.\")\n",
    "print(f\"Validation: {len(base_dataset['validation'])} items.\")\n",
    "print(f\"Testing: {len(base_dataset['testing'])} items.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Documents\\Mackenzie\\TCC\\.venv\\Lib\\site-packages\\transformers\\models\\vit\\feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "classifier = SkinConditionsClassifier(parameters['TRAINING']['pretrained_model_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Healthy skin precision: 100.0%\n",
      "Diseased skin precision: 56.449771689497716%\n",
      "Total precision: 72.80826799714897%\n"
     ]
    }
   ],
   "source": [
    "precision_healthy = 0\n",
    "precision_diseased = 0\n",
    "\n",
    "for img in raw_healthy_images:\n",
    "    prediction_score = flatten_prediction(classifier.predict(PIL.Image.fromarray(img))).get(parameters['TRAINING']['target_label'], 0)\n",
    "    \n",
    "    if prediction_score < parameters['TRAINING']['threshold']:\n",
    "        precision_healthy += 1\n",
    "        \n",
    "for img in raw_diseased_images:\n",
    "    prediction_score = flatten_prediction(classifier.predict(PIL.Image.fromarray(img))).get(parameters['TRAINING']['target_label'], 0)\n",
    "    \n",
    "    if prediction_score >= parameters['TRAINING']['threshold']:\n",
    "        precision_diseased += 1\n",
    "\n",
    "total_precision = (precision_healthy + precision_diseased) / len(raw_healthy_images + raw_diseased_images)\n",
    "precision_healthy /= len(raw_healthy_images)\n",
    "precision_diseased /= len(raw_diseased_images)\n",
    "\n",
    "print(f'Healthy skin precision: {precision_healthy*100}%')\n",
    "print(f'Diseased skin precision: {precision_diseased*100}%')\n",
    "print(f'Total precision: {total_precision*100}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "train_dataset = SkinDiseaseDataset('/dataset', transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BinaryCNN()\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "train_model(model, train_loader, criterion, optimizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
