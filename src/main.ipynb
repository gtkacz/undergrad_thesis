{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <p align=\"center\">\n",
    "        <img src=\"https://logodownload.org/wp-content/uploads/2017/09/mackenzie-logo-3.png\" style=\"height: 7ch;\"><br>\n",
    "        <h1 align=\"center\">Computer Systems Undergradute Thesis</h1>\n",
    "        <h2 align=\"center\">Quantitative Analysis of the Impact of Image Pre-Processing on the Accuracy of Computer Vision Models Trained to Identify Dermatological Skin Diseases</a>\n",
    "        <h4 align=\"center\">Gabriel Mitelman Tkacz</a>\n",
    "        </h4>\n",
    "    </p>\n",
    "</center>\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tomllib\n",
    "from functools import partial\n",
    "from pprint import pprint\n",
    "from typing import Sequence\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from pynimbar import loading_animation\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "from util import (\n",
    "    BinaryCNN,\n",
    "    LossFunction,\n",
    "    NormalizeTransform,\n",
    "    SkinDiseaseDataset,\n",
    "    TestingDataset,\n",
    "    TrainingDataset,\n",
    "    ValidationDataset,\n",
    "    evaluate,\n",
    "    split_datasets,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'TRAINING': {'batch_size': 128,\n",
      "              'diseased_skin_path': './dataset/diseased/',\n",
      "              'healthy_skin_path': './dataset/healthy/',\n",
      "              'learning_rate': 0.0001,\n",
      "              'num_epochs': 3,\n",
      "              'num_workers': 12,\n",
      "              'pin_memory': True,\n",
      "              'precision_threshold': 0.8,\n",
      "              'shuffle': True,\n",
      "              'training_dataset_ratio': 0.8}}\n"
     ]
    }
   ],
   "source": [
    "with open(\"parameters.toml\", \"r\") as f:\n",
    "    parameters = tomllib.loads(f.read())\n",
    "\n",
    "loading_handler = partial(\n",
    "    loading_animation, break_on_error=True, verbose_errors=True, time_it_live=True\n",
    ")\n",
    "\n",
    "pprint(parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ratio: 80.0%\n",
      "Testing ratio: 10.0%\n",
      "Validation ratio: 10.0%\n",
      "\n",
      "Seed: 47\n"
     ]
    }
   ],
   "source": [
    "training_ratio = parameters[\"TRAINING\"][\"training_dataset_ratio\"]\n",
    "testing_ratio = validation_ratio = round(1 - training_ratio, 1) / 2\n",
    "\n",
    "print(f\"Training ratio: {training_ratio*100}%\")\n",
    "print(f\"Testing ratio: {testing_ratio*100}%\")\n",
    "print(f\"Validation ratio: {validation_ratio*100}%\")\n",
    "\n",
    "seed = 47\n",
    "print(f\"\\nSeed: {seed}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_data(\n",
    "    to_transforms: Sequence[nn.Module] = list(),\n",
    ") -> tuple[\n",
    "    TrainingDataset,\n",
    "    DataLoader,\n",
    "    TestingDataset,\n",
    "    DataLoader,\n",
    "    ValidationDataset,\n",
    "    DataLoader,\n",
    "]:\n",
    "    \"\"\"\n",
    "    This function returns the training and testing data loaders and datasets for the skin disease dataset.\n",
    "\n",
    "    Args:\n",
    "        to_transforms (Sequence[nn.Module], optional): A sequence of transforms to apply to the dataset. Defaullts to an empty sequence.\n",
    "\n",
    "    Returns:\n",
    "        dict[str, dict[str, DataLoader | SkinDiseaseDataset]]: A dictionary containing the training and testing data loaders and datasets.\n",
    "    \"\"\"\n",
    "    base_transforms = [transforms.Resize((128, 128)), transforms.ToTensor()]\n",
    "    transform = transforms.Compose([*base_transforms, *to_transforms])\n",
    "\n",
    "    loader_kwargs = {\n",
    "        \"batch_size\": parameters[\"TRAINING\"][\"batch_size\"],\n",
    "        \"shuffle\": parameters[\"TRAINING\"][\"shuffle\"],\n",
    "        \"num_workers\": parameters[\"TRAINING\"][\"num_workers\"],\n",
    "        \"pin_memory\": parameters[\"TRAINING\"][\"pin_memory\"],\n",
    "    }\n",
    "\n",
    "    base_dataset = SkinDiseaseDataset(root_dir=\"dataset\", transform=transform)\n",
    "    train_dataset, test_dataset, validation_dataset = split_datasets(\n",
    "        base_dataset, training_ratio, testing_ratio, validation_ratio, seed\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, **loader_kwargs)\n",
    "    test_loader = DataLoader(test_dataset, **loader_kwargs)\n",
    "    validation_loader = DataLoader(validation_dataset, **loader_kwargs)\n",
    "\n",
    "    return (\n",
    "        train_dataset,\n",
    "        train_loader,\n",
    "        test_dataset,\n",
    "        test_loader,\n",
    "        validation_dataset,\n",
    "        validation_loader,\n",
    "    )\n",
    "\n",
    "\n",
    "def evaluate_model(\n",
    "    device: torch.device,\n",
    "    train_loader: DataLoader,\n",
    "    test_loader: DataLoader,\n",
    "    validation_loader: DataLoader,\n",
    "    criterion: LossFunction = nn.MSELoss(),\n",
    "    optimizer_class: type[optim.Optimizer] = optim.Adam,\n",
    "    learning_rate: float = parameters[\"TRAINING\"][\"learning_rate\"],\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    This function evaluates the model using the given criterion and data loaders.\n",
    "\n",
    "    Args:\n",
    "        device (torch.device): The device to use for the evaluation.\n",
    "        train_loader (DataLoader): The training data loader.\n",
    "        test_loader (DataLoader): The testing data loader.\n",
    "        validation_loader (DataLoader): The validation data loader.\n",
    "        criterion (LossFunction): The loss function to use for the evaluation. Defaults to nn.BCELoss().\n",
    "        optimizer_class (type[optim.Optimizer], optional): The optimizer class to use for the evaluation. Defaults to optim.Adagrad.\n",
    "\n",
    "    Returns:\n",
    "        float: The accuracy of the model.\n",
    "    \"\"\"\n",
    "    criterion = criterion.to(device)\n",
    "\n",
    "    model = BinaryCNN(device=device).to(device)\n",
    "    optimizer = optimizer_class(model.parameters(), lr=learning_rate)  # type: ignore\n",
    "\n",
    "    return evaluate(\n",
    "        model=model,\n",
    "        criterion=criterion,\n",
    "        device=device,\n",
    "        verbose=True,\n",
    "        optimizer=optimizer,\n",
    "        train_loader=train_loader,\n",
    "        test_loader=test_loader,\n",
    "        validation_loader=validation_loader,\n",
    "        num_epochs=parameters[\"TRAINING\"][\"num_epochs\"],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class 0 Model: Images with no pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Epoch 1/3, Train Loss: 0.0807, Train Accuracy: 89.44%, Validation Loss: 0.2306, Validation Accuracy: 49.00%\n",
      "Best model saved at epoch 1 with Validation Accuracy: 49.00%\n",
      "Epoch 2/3, Train Loss: 0.0364, Train Accuracy: 95.31%, Validation Loss: 0.1139, Validation Accuracy: 87.00%\n",
      "Best model saved at epoch 2 with Validation Accuracy: 87.00%\n",
      "Epoch 3/3, Train Loss: 0.0294, Train Accuracy: 96.06%, Validation Loss: 0.0678, Validation Accuracy: 89.50%\n",
      "Best model saved at epoch 3 with Validation Accuracy: 89.50%\n",
      "Total training duration: 3.48 minutes\n",
      "Test Accuracy of the Binary Classification Model: 89.50%\n",
      "Base precision: 89.50%\n"
     ]
    }
   ],
   "source": [
    "(\n",
    "    base_train_dataset,\n",
    "    base_train_loader,\n",
    "    base_test_dataset,\n",
    "    base_test_loader,\n",
    "    base_validation_dataset,\n",
    "    base_validation_loader,\n",
    ") = get_model_data()\n",
    "\n",
    "base_precision = evaluate_model(\n",
    "    device, base_train_loader, base_test_loader, base_validation_loader\n",
    ")\n",
    "\n",
    "print(f\"Base precision: {base_precision*100:.2f}%\")\n",
    "\n",
    "if base_precision < parameters[\"TRAINING\"][\"precision_threshold\"]:\n",
    "    raise ValueError(\"The base model did not meet the precision threshold.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class 1 Models: Images with only one pre-process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class 1.1 Models: Normalizing the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Epoch 1/3, Train Loss: 0.0746, Train Accuracy: 90.88%, Validation Loss: 0.2002, Validation Accuracy: 66.00%\n",
      "Best model saved at epoch 1 with Validation Accuracy: 66.00%\n",
      "Epoch 2/3, Train Loss: 0.0312, Train Accuracy: 95.88%, Validation Loss: 0.0663, Validation Accuracy: 91.00%\n",
      "Best model saved at epoch 2 with Validation Accuracy: 91.00%\n",
      "Epoch 3/3, Train Loss: 0.0258, Train Accuracy: 96.50%, Validation Loss: 0.0466, Validation Accuracy: 93.50%\n",
      "Best model saved at epoch 3 with Validation Accuracy: 93.50%\n",
      "Total training duration: 3.43 minutes\n",
      "Test Accuracy of the Binary Classification Model: 92.00%\n",
      "Normalized precision: 92.00%\n",
      "That is an upgrade of 0.025000000000000022.\n"
     ]
    }
   ],
   "source": [
    "(\n",
    "    normalize_train_dataset,\n",
    "    normalize_train_loader,\n",
    "    normalize_test_dataset,\n",
    "    normalize_test_loader,\n",
    "    normalize_validation_dataset,\n",
    "    normalize_validation_loader,\n",
    ") = get_model_data([NormalizeTransform()])\n",
    "\n",
    "normalize_precision = evaluate_model(\n",
    "    device, normalize_train_loader, normalize_test_loader, normalize_validation_loader\n",
    ")\n",
    "\n",
    "normalize_precision_diff = normalize_precision - base_precision\n",
    "\n",
    "print(f\"Normalized precision: {normalize_precision*100:.2f}%\")\n",
    "print(\n",
    "    f\"That is an {'upgrade' if normalize_precision_diff > 0 else 'downgrade'} of {normalize_precision_diff*100:.2f}%.\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
